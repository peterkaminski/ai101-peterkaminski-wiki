# AI 101 Open Session, 2024-01-26a

YouTube: <https://youtu.be/_6HZB1PNT3c>

Recorded live AI Salon / AI 101 session on AI topics.

**Next:** [[AI 101 Open Session, 2024-01-26b]]

**Previous:** [[AI 101 Open Session, 2024-01-23]]

## AI Summary (hopefully useful, may be inaccurate)

### Quick Recap

Pete Kaminski introduced Llamafile, a large language model that can be downloaded as a single file, and discussed its potential applications such as image description. He highlighted that Llama could be used with other models and its file size is relatively small at 4GB. Challenges of shopping for large language open-source models were also discussed, with questions asked about the model's size and capabilities. Pete shared insights on various models and tools, including the mixture of experts model, Mistral, and Mixed Draw models. The idea of having a smaller version of Mem Gpt that could run on a personal computer was also discussed.
### Summary

**Llama Language Model Demonstration**

Pete Kaminski demonstrated a large language model, Llama, that can be downloaded as a single file. The model was discussed in relation to its ability to describe images and its potential applications. Pete also mentioned that the model could be used with different models, and that the size of the model file was relatively small at 4GB. R J and Dr. Dr. Claire Jacobs participated in the discussion, asking questions about the model's size and capabilities. Pete shared a link to the model's source code and instructions on how to download and use it.

**Large Language Open-Source Model Challenges Discussed**

R J and Pete Kaminski discussed the challenges of shopping for large language open-source models. They explored the complexities of the ecosystem, the lack of well-documented resources, and the difficulty of comparing new models. Pete shared his insights on the mixture of experts model and its application in Gpt 4, which he suggested was a significant development in the field. They also clarified the difference between Mistral and Mixed Draw models, with Pete noting the potential for confusion due to similar architecture and version numbers.

**Searching for Information Online**

Pete Kaminski discussed the process of searching for specific information online and stressed the importance of refining search queries. He shared his experience with a friend who struggles with technology updates and recommended YouTube and Reddit for learning. R J then suggested using language models, with Pete explaining the concept and recommending a tool called Lank chain. Pete also mentioned an expert, Simon Willison, who has developed a tool similar to others and shared that his blog posts and tools have been beneficial for others. However, no explicit decision was made about bringing Simon on board for their project.

**Smaller Mem GPT for Personal Use and Writing Aids**

R J and Pete Kaminski discussed the idea of having a smaller version of Mem Gpt that could run on a personal computer. They also discussed the use of the Mem Gpt for writers to keep track of their plots, characters, and other notes. Pete demonstrated how to install Mem Gpt using Pip and how to run it using the free Mem Gpt endpoints. They also discussed the potential of creating a custom LLM endpoint. Pete suggested that the Mem Gpt could be running in the cloud, sponsored by someone.

**Terminal Settings and Agent Development**

Pete Kaminski was discussing the settings of his terminal program, with R J suggesting a possible solution to his issue. Pete also mentioned his plans to create an agent that remembers previous interactions and can be asked about them. While Pete was optimistic about this feature, he also pointed out a potential problem where the agent failed to generate new responses. R J agreed with Pete's assessment that this was more of a process bug than a hallucination.

**Memgpt Model Selection Challenges**

Pete Kaminski and R J discussed the challenges of setting up and selecting models for a system called memgpt. They highlighted the dependencies and requirements needed to get the system running, including a model wrapper and an AI API. They also emphasized the importance of seeking advice from experienced individuals or communities, like writers, who have successfully used and tested these models. Pete suggested testing the system with Gpt 4 via the OpenAI API. They also briefly discussed the potential for future improvements, such as local GPT 4 and GPT 5 models. Towards the end, they briefly mentioned an incident where Kyle shared his entire credit card number.

**OpenAI's GPT-4 Model for Language Learning**

Pete Kaminski and his team discussed the use of OpenAI's GPT-4 model for language learning. They experimented with different settings, including selecting the storage backend for archival data. Pete highlighted the model's internal dialogue and its potential for conversational language learning. The team also contemplated the idea of using cloud storage for archival data. However, due to time constraints, they did not delve deeper into some topics. Towards the end, Dr. Dr. Claire Jacobs had to leave the meeting.